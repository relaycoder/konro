uuid: 421cf192-b258-4da5-b040-d9d10e081395
projectId: konro
createdAt: '2025-07-09T04:31:29.728Z'
linesAdded: 268
linesRemoved: 21
gitCommitMsg: 'feat: Add ''per-record'' file storage strategy'
promptSummary: >-
  The user wants to add a new file storage strategy called 'per-record' where
  each record is a separate file. This should be available in both 'in-memory'
  and 'on-demand' modes and support JSON or YAML formats.
reasoning:
  - >-
    This is a great feature request! Storing each record in its own file is a
    powerful strategy, especially for `on-demand` access patterns with large
    datasets, as it avoids reading and writing entire table files for small
    changes. Here is the implementation plan:
  - '1.  **Update `src/adapter.ts`**:'
  - >-
    *   I'll introduce a new `perRecord` strategy to `FileAdapterOptions`. This
    strategy will store each database record in a separate file within a
    directory structure of `{baseDir}/{tableName}/{recordId}.{format}`.
  - >-
    *   This strategy will be limited to `json` and `yaml` formats, as tabular
    formats like CSV/XLSX don't make sense for single records.
  - >-
    *   I will modify the `StorageAdapter` interface, changing the `write`
    method signature to `write(state: DatabaseState<any>, schema:
    KonroSchema<any, any>)`. This is necessary for the `per-record` writer to
    access the schema and identify the primary key column for filenames.
  - >-
    *   For the `in-memory` mode, I'll implement `readPerRecord` and
    `writePerRecord` functions. These will handle reading a directory of records
    into the database state and writing the state back out to individual files,
    including a `_meta.json` file in each table directory to track `lastId` for
    auto-incrementing keys.
  - '2.  **Update `src/db.ts`**:'
  - >-
    *   To support the new `per-record` strategy in `on-demand` mode, I will
    refactor the existing `on-demand` logic. The current implementation, which
    is tightly coupled to the `multi-file` (one file per table) strategy, will
    be renamed to `createMultiFileOnDemandDbContext`.
  - >-
    *   I will create a new, separate `createPerRecordOnDemandDbContext`. This
    new function will contain optimized logic for `per-record` storage:
  - '*   `insert`: Will create new record files directly.'
  - >-
    *   `update` and `delete`: Will operate on specific record files after
    reading the necessary records from the table's directory to perform
    validation.
  - >-
    *   `query`: For queries involving relations (`with`), it will still need to
    load the data from all relevant tables, similar to the multi-file approach,
    to ensure correctness.
  - >-
    *   The main `createDatabase` function will be updated to detect the chosen
    file strategy and instantiate the correct `on-demand` context (`multi-file`
    or `per-record`).
  - >-
    *   Finally, I'll adjust the `InMemoryDbContext` to pass the `schema` to the
    adapter's `write` method, accommodating the signature change from step 1.
  - Let's begin with the adapter changes.
  - '### 1. Modify `src/adapter.ts` for the `per-record` strategy'
  - >-
    I'll start by updating `FileAdapterOptions` to include the `perRecord`
    strategy and make the strategy options mutually exclusive. Then, I'll change
    the `StorageAdapter` interface and implement the `read` and `write` logic
    for this new mode.
  - '### 2. Refactor and Extend `src/db.ts`'
  - >-
    Now I'll modify the database context creation. I'll update the
    `InMemoryDbContext` to pass the schema to `write`, rename the existing
    on-demand context, and create the new `per-record` on-demand context.
  - >-
    The changes are complete. I have introduced the `per-record` storage
    strategy which works in both `in-memory` and `on-demand` modes. The
    implementation ensures that file operations are handled efficiently for the
    on-demand case, while still providing a simple read/write mechanism for the
    in-memory case. This new strategy significantly enhances the flexibility of
    Konro for different use cases and data sizes.
operations:
  - type: write
    path: src/adapter.ts
    content: >-
      --- src/adapter.ts

      +++ src/adapter.ts

      @@ -1,10 +1,11 @@
       import path from 'path';
      -import type { DatabaseState, TableState } from './types';

      +import type { DatabaseState, KRecord, TableState } from './types';
       import { createEmptyState } from './operations';
       import type { ColumnDefinition, KonroSchema } from './schema';
       import { type Serializer, getSerializer } from './utils/serializer.util';
       import { FsProvider, defaultFsProvider, writeAtomic } from './fs';
       import { KonroError, KonroStorageError } from './utils/error.util';
      +import { TEMP_FILE_SUFFIX } from './utils/constants';
       
       export interface StorageAdapter {
         read<S extends KonroSchema<any, any>>(schema: S): Promise<DatabaseState<S>>;
      -  write(state: DatabaseState<any>): Promise<void>;

      +  write(state: DatabaseState<any>, schema: KonroSchema<any, any>):
      Promise<void>;
         readonly mode: 'in-memory' | 'on-demand';
       }
       
      @@ -15,18 +16,19 @@
         readonly fileExtension: string;
       }
       
      -type SingleFileStrategy = { single: { filepath: string }; multi?: never
      };

      -type MultiFileStrategy = { multi: { dir: string }; single?: never };

      +type SingleFileStrategy = { single: { filepath: string }; multi?: never;
      perRecord?: never };

      +type MultiFileStrategy = { multi: { dir: string }; single?: never;
      perRecord?: never };

      +type PerRecordStrategy = { perRecord: { dir: string }; single?: never;
      multi?: never };
       
       export type FileAdapterOptions = {
         format: 'json' | 'yaml' | 'csv' | 'xlsx';
         fs?: FsProvider;
         /**
          * Defines the data access strategy.
      -   * - `in-memory`: (Default) Loads the entire database into memory on
      read. Fast for small/medium datasets.

      -   * - `on-demand`: Reads from the file system for each query. Slower but
      supports larger datasets. Requires the 'multi-file' strategy.

      +   * - `in-memory`: (Default) Loads the entire database into memory on
      init. Fast for small/medium datasets.

      +   * - `on-demand`: Reads from the file system for each query. Slower but
      supports larger datasets. Requires 'multi-file' or 'per-record' strategy.
          */
         mode?: 'in-memory' | 'on-demand';
      -} & (SingleFileStrategy | MultiFileStrategy);

      +} & (SingleFileStrategy | MultiFileStrategy | PerRecordStrategy);
       
       export function createFileAdapter(options: FileAdapterOptions & { mode: 'on-demand' }): FileStorageAdapter & { mode: 'on-demand' };
       export function createFileAdapter(options: FileAdapterOptions & { mode?: 'in-memory' | undefined }): FileStorageAdapter & { mode: 'in-memory' };
      @@ -42,8 +44,12 @@
           throw KonroError(`The '${options.format}' format only supports 'on-demand' mode with a 'multi-file' strategy.`);
         }
       
      +  if (options.perRecord && options.format !== 'json' && options.format
      !== 'yaml') {

      +    throw KonroError(`The 'per-record' strategy only supports 'json' or
      'yaml' formats.`);

      +  }

      +
         if (mode === 'on-demand' && options.single) {
      -    throw KonroError("The 'on-demand' mode requires the 'multi-file'
      storage strategy.");

      +    throw KonroError("The 'on-demand' mode requires the 'multi-file' or
      'per-record' storage strategy.");
         }
       
         const parseFile = async <T>(filepath: string, schema?: Record<string, ColumnDefinition<any>>): Promise<T | undefined> => {
      @@ -75,15 +81,95 @@
           return state;
         };
       
      -  const writeSingle = (state: DatabaseState<any>) =>
      writeAtomic(options.single!.filepath, serializer.stringify(state), fs);

      +  const readPerRecord = async <S extends KonroSchema<any, any>>(schema:
      S): Promise<DatabaseState<S>> => {

      +    const dir = options.perRecord!.dir;

      +    await fs.mkdir(dir, { recursive: true });

      +    const state = createEmptyState(schema);

      +

      +    await Promise.all(

      +      Object.keys(schema.tables).map(async (tableName) => {

      +        const tableDir = path.join(dir, tableName);

      +        await fs.mkdir(tableDir, { recursive: true });

      +

      +        // Read meta file for lastId

      +        const metaPath = path.join(tableDir, '_meta.json');

      +        try {

      +          const metaContent = await fs.readFile(metaPath);

      +          if (metaContent) {

      +            (state as any)[tableName].meta = JSON.parse(metaContent);

      +          }

      +        } catch (e) {

      +          /* ignore if not found or parsing fails, will use default */

      +        }

      +

      +        const files = await fs.readdir(tableDir);

      +        const recordFiles = files.filter((f) => !f.startsWith('_meta'));

      +

      +        const records = await Promise.all(

      +          recordFiles.map(async (file) => {

      +            const recordPath = path.join(tableDir, file);

      +            const recordContent = await fs.readFile(recordPath);

      +            if (!recordContent) return null;

      +            // The serializer for json/yaml just parses the content,
      schema is ignored.

      +            return serializer.parse<KRecord>(recordContent);

      +          })

      +        );

      +

      +        (state as any)[tableName].records = records.filter((r) => r !==
      null);

      +

      +        // If meta file didn't exist or was empty, derive lastId for
      auto-increment PKs.

      +        if ((state as any)[tableName].meta.lastId === 0) {

      +          const tableSchema = schema.tables[tableName];

      +          const idColumn = Object.keys(tableSchema).find((key) =>
      tableSchema[key]?.dataType === 'id' &&
      tableSchema[key]?.options?._pk_strategy !== 'uuid');

      +          if (idColumn) {

      +            (state as any)[tableName].meta.lastId = (state as
      any)[tableName].records.reduce((maxId: number, record: KRecord) => {

      +              const id = record[idColumn];

      +              return typeof id === 'number' && id > maxId ? id : maxId;

      +            }, 0);

      +          }

      +        }

      +      })

      +    );

      +    return state;

      +  };

      +

      +  const writeSingle = (state: DatabaseState<any>) =>
      writeAtomic(options.single!.filepath, serializer.stringify(state), fs);
       
      -  const writeMulti = async (state: DatabaseState<any>) => {

      +  const writeMulti = async (state: DatabaseState<any>) => {
           const dir = options.multi!.dir;
           await fs.mkdir(dir, { recursive: true });
           const writes = Object.entries(state).map(([tableName, tableState]) => {
             const filepath = path.join(dir, `${tableName}${fileExtension}`);
             return writeAtomic(filepath, serializer.stringify(tableState), fs);
           });
      +    await Promise.all(writes);

      +  };

      +

      +  const writePerRecord = async (state: DatabaseState<any>, schema:
      KonroSchema<any, any>) => {

      +    const dir = options.perRecord!.dir;

      +    await fs.mkdir(dir, { recursive: true });

      +

      +    const writes = Object.entries(state).map(async ([tableName,
      tableState]) => {

      +      const tableDir = path.join(dir, tableName);

      +      await fs.mkdir(tableDir, { recursive: true });

      +

      +      // Write meta file first

      +      const metaPath = path.join(tableDir, '_meta.json');

      +      await writeAtomic(metaPath, JSON.stringify(tableState.meta, null,
      2), fs);

      +

      +      const idColumn = Object.keys(schema.tables[tableName]).find((key)
      => schema.tables[tableName][key]?.dataType === 'id');

      +      if (!idColumn) {

      +        throw KonroError(`Table "${tableName}" must have an 'id' column
      to be used with 'per-record' storage.`);

      +      }

      +

      +      const currentFiles = new Set(tableState.records.map((r: KRecord) =>
      `${r[idColumn]}${fileExtension}`));

      +      const existingFiles = (await fs.readdir(tableDir)).filter((f) =>
      !f.startsWith('_meta') && !f.endsWith(TEMP_FILE_SUFFIX));

      +

      +      const recordWrites = tableState.records.map((record: KRecord) =>
      writeAtomic(path.join(tableDir, `${record[idColumn]}${fileExtension}`),
      serializer.stringify(record), fs));

      +      const recordsToDelete = existingFiles.filter((f) =>
      !currentFiles.has(f));

      +      const recordDeletes = recordsToDelete.map((f) =>
      fs.unlink(path.join(tableDir, f)));

      +

      +      await Promise.all([...recordWrites, ...recordDeletes]);

      +    });
           await Promise.all(writes);
         };
       
      @@ -93,7 +179,7 @@
           serializer,
           fileExtension,
           mode,
      -    read: options.single ? readSingle : readMulti,

      -    write: options.single ? writeSingle : writeMulti,

      +    read: options.single ? readSingle : options.multi ? readMulti :
      readPerRecord,

      +    write: options.single ? writeSingle : options.multi ? writeMulti :
      writePerRecord,
         } as FileStorageAdapter;
       }
    patchStrategy: new-unified
  - type: write
    path: src/db.ts
    content: >-
      --- src/db.ts

      +++ src/db.ts

      @@ -107,7 +107,7 @@
         schema: S;
         adapter: StorageAdapter;
         read(): Promise<DatabaseState<S>>;
      -  write(state: DatabaseState<S>): Promise<void>;

      +  write(state: DatabaseState<S>): Promise<void>;
         createEmptyState(): DatabaseState<S>;
       
         query(state: DatabaseState<S>): QueryBuilder<S>;
      @@ -194,14 +194,14 @@
       
       // --- ON-DEMAND CONTEXT (STATEFUL WRAPPER) ---
       
      -function createOnDemandDbContext<S extends KonroSchema<any, any>>(

      +function createMultiFileOnDemandDbContext<S extends KonroSchema<any,
      any>>(
         schema: S,
         adapter: FileStorageAdapter,
         core: ReturnType<typeof createCoreDbContext<S>>
       ): OnDemandDbContext<S> {
         const { dir } = adapter.options.multi!;
       
      -  const readTable = async (tableName: string): Promise<TableState> => {

      +  const readTableState = async (tableName: string): Promise<TableState>
      => {
           const filepath = path.join(dir, `${tableName}${adapter.fileExtension}`);
           const data = await adapter.fs.readFile(filepath);
           if (!data) return { records: [], meta: { lastId: 0 } };
      @@ -212,7 +212,7 @@
           }
         };
       
      -  const writeTable = async (tableName: string, tableState: TableState):
      Promise<void> => {

      +  const writeTableState = async (tableName: string, tableState:
      TableState): Promise<void> => {
           await adapter.fs.mkdir(dir, { recursive: true });
           const filepath = path.join(dir, `${tableName}${adapter.fileExtension}`);
           const content = adapter.serializer.stringify(tableState);
      @@ -223,14 +223,14 @@
         const getFullState = async (): Promise<DatabaseState<S>> => {
           const state = createEmptyStateImpl(schema);
           await Promise.all(Object.keys(schema.tables).map(async (tableName) => {
      -      (state as any)[tableName] = await readTable(tableName);

      +      (state as any)[tableName] = await readTableState(tableName);
           }));
           return state;
         }
       
         // A generic handler for CUD operations that reads one table, performs an action, and writes it back.
         const performCud = async <TResult>(tableName: string, action: (state: DatabaseState<S>) => [DatabaseState<S>, TResult]): Promise<TResult> => {
      -    const state = createEmptyStateImpl(schema);

      +    const state = createEmptyStateImpl(schema) as DatabaseState;
           (state as any)[tableName] = await readTable(tableName);
           const [newState, result] = action(state);
           
      @@ -241,7 +241,7 @@
             // This check satisfies the `noUncheckedIndexedAccess` compiler option.
             // Our CUD logic ensures this state will always exist after a change.
             if (newTableState) {
      -        await writeTable(tableName, newTableState);

      +        await writeTableState(tableName, newTableState);
             }
           }
           return result;
      @@ -289,6 +289,161 @@
         };
       }
       
      +function createPerRecordOnDemandDbContext<S extends KonroSchema<any,
      any>>(

      +  schema: S,

      +  adapter: FileStorageAdapter,

      +  core: ReturnType<typeof createCoreDbContext<S>>

      +): OnDemandDbContext<S> {

      +  const { dir } = adapter.options.perRecord!;

      +  const { fs, serializer, fileExtension } = adapter;

      +

      +  const getTableDir = (tableName: string) => path.join(dir, tableName);

      +  const getRecordPath = (tableName: string, recordId: string | number) =>
      path.join(getTableDir(tableName), `${recordId}${fileExtension}`);

      +  const getMetaPath = (tableName: string) =>
      path.join(getTableDir(tableName), '_meta.json');

      +

      +  const getIdColumn = (tableName: string) => {

      +    const tableSchema = schema.tables[tableName];

      +    const idColumn = Object.keys(tableSchema).find((key) =>
      tableSchema[key]?.dataType === 'id');

      +    if (!idColumn) {

      +      throw KonroError(`Table "${tableName}" must have an 'id' column to
      be used with 'per-record' storage.`);

      +    }

      +    return idColumn;

      +  };

      +

      +  const readMeta = async (tableName: string): Promise<{ lastId: number }>
      => {

      +    const metaContent = await fs.readFile(getMetaPath(tableName));

      +    return metaContent ? JSON.parse(metaContent) : { lastId: 0 };

      +  };

      +

      +  const writeMeta = async (tableName: string, meta: { lastId: number }):
      Promise<void> => {

      +    await fs.mkdir(getTableDir(tableName), { recursive: true });

      +    await writeAtomic(getMetaPath(tableName), JSON.stringify(meta, null,
      2), fs);

      +  };

      +

      +  const readTableState = async (tableName: string): Promise<TableState>
      => {

      +    const tableDir = getTableDir(tableName);

      +    await fs.mkdir(tableDir, { recursive: true });

      +

      +    const meta = await readMeta(tableName);

      +    const files = await fs.readdir(tableDir);

      +    const recordFiles = files.filter((f) => !f.startsWith('_meta'));

      +

      +    const records = (

      +      await Promise.all(

      +        recordFiles.map(async (file) => {

      +          const content = await fs.readFile(path.join(tableDir, file));

      +          return content ? serializer.parse<KRecord>(content) : null;

      +        })

      +      )

      +    ).filter((r): r is KRecord => r !== null);

      +

      +    return { records, meta };

      +  };

      +

      +  const getFullState = async (): Promise<DatabaseState<S>> => {

      +    const state = createEmptyStateImpl(schema);

      +    await Promise.all(

      +      Object.keys(schema.tables).map(async (tableName) => {

      +        (state as any)[tableName] = await readTableState(tableName);

      +      })

      +    );

      +    return state;

      +  };

      +

      +  const query = (): OnDemandQueryBuilder<S> => ({

      +    from: <TName extends keyof S['tables']>(tableName: TName):
      OnDemandChainedQueryBuilder<S, TName, S['base'][TName]> => {

      +      const createBuilder = <TReturn>(currentDescriptor:
      QueryDescriptor): OnDemandChainedQueryBuilder<S, TName, TReturn> => ({

      +        select(fields) { return createBuilder<TReturn>({
      ...currentDescriptor, select: fields }); },

      +        where(predicate) { return createBuilder<TReturn>({
      ...currentDescriptor, where: normalizePredicate(predicate as any) }); },

      +        with<W extends WithArgument<S['types'][TName]>>(relations: W) {

      +          const newWith = { ...currentDescriptor.with, ...(relations as
      QueryDescriptor['with']) };

      +          return createBuilder<TReturn & ResolveWith<S, TName, W>>({
      ...currentDescriptor, with: newWith });

      +        },

      +        limit(count) { return createBuilder<TReturn>({
      ...currentDescriptor, limit: count }); },

      +        offset(count) { return createBuilder<TReturn>({
      ...currentDescriptor, offset: count }); },

      +        all: async (): Promise<TReturn[]> => {

      +          const state = await getFullState();

      +          return _queryImpl(state, schema, currentDescriptor) as any;

      +        },

      +        first: async (): Promise<TReturn | null> => {

      +          const state = await getFullState();

      +          return (_queryImpl(state, schema, { ...currentDescriptor,
      limit: 1 })[0] ?? null) as any;

      +        },

      +        aggregate: async (aggregations) => {

      +          const state = await getFullState();

      +          const aggDescriptor: AggregationDescriptor = {
      ...currentDescriptor, aggregations };

      +          return _aggregateImpl(state, schema, aggDescriptor) as any;

      +        },

      +      });

      +      return createBuilder<S['base'][TName]>({ tableName: tableName as
      string });

      +    },

      +  });

      +

      +  const insert = async <T extends keyof S['tables']>(tableName: T,
      values: S['create'][T] | Readonly<S['create'][T]>[]): Promise<any> => {

      +    const tableNameStr = tableName as string;

      +    const meta = await readMeta(tableNameStr);

      +    const idColumn = getIdColumn(tableNameStr);

      +

      +    // We only need a shallow table state for insert, no records needed
      for validation context.

      +    const tempState: DatabaseState = { [tableNameStr]: { records: [],
      meta } };

      +    const [newState, insertedRecords] = core.insert(tempState as any,
      tableName, values as any);

      +

      +    if (!insertedRecords || insertedRecords.length === 0) {

      +      return Array.isArray(values) ? [] : null;

      +    }

      +

      +    await Promise.all(

      +      (insertedRecords as KRecord[]).map((rec) => {

      +        const recordPath = getRecordPath(tableNameStr, rec[idColumn] as
      any);

      +        return writeAtomic(recordPath, serializer.stringify(rec), fs);

      +      })

      +    );

      +

      +    const newMeta = (newState as DatabaseState)[tableNameStr]?.meta;

      +    if (newMeta && newMeta.lastId !== meta.lastId) {

      +      await writeMeta(tableNameStr, newMeta);

      +    }

      +

      +    return Array.isArray(values) ? insertedRecords : insertedRecords[0];

      +  };

      +

      +  const update = <T extends keyof S['tables']>(tableName: T):
      OnDemandUpdateBuilder<S['base'][T], S['create'][T]> => ({

      +    set: (data) => ({

      +      where: async (predicate) => {

      +        const tableNameStr = tableName as string;

      +        const tableState = await readTableState(tableNameStr);

      +        const idColumn = getIdColumn(tableNameStr);

      +        const [, updatedRecords] = core.update({ [tableNameStr]:
      tableState } as any, tableName).set(data).where(predicate);

      +

      +        if (updatedRecords.length > 0) {

      +          await Promise.all(

      +            (updatedRecords as KRecord[]).map((rec) =>
      writeAtomic(getRecordPath(tableNameStr, rec[idColumn] as any),
      serializer.stringify(rec), fs))

      +          );

      +        }

      +        return updatedRecords as S['base'][T][];

      +      },

      +    }),

      +  });

      +

      +  const del = <T extends keyof S['tables']>(tableName: T):
      OnDemandDeleteBuilder<S['base'][T]> => ({

      +    where: async (predicate) => {

      +      const tableNameStr = tableName as string;

      +      const tableState = await readTableState(tableNameStr);

      +      const idColumn = getIdColumn(tableNameStr);

      +      const [, deletedRecords] = core.delete({ [tableNameStr]: tableState
      } as any, tableName, predicate as any);

      +

      +      if (deletedRecords.length > 0) {

      +        await Promise.all((deletedRecords as KRecord[]).map((rec) =>
      fs.unlink(getRecordPath(tableNameStr, rec[idColumn] as any))));

      +      }

      +      return deletedRecords as S['base'][T][];

      +    },

      +  });

      +

      +  const notSupported = () => Promise.reject(KonroError("This method is
      not supported in 'on-demand' mode."));

      +

      +  return { schema, adapter, createEmptyState: () =>
      createEmptyStateImpl(schema), read: notSupported, write: notSupported,
      query, insert, update, delete: del };

      +}

      +
       
       // --- DATABASE FACTORY ---
       
      @@ -303,16 +458,20 @@
         const core = createCoreDbContext(schema);
       
         if (adapter.mode === 'on-demand') {
      -    // We can be reasonably sure it's a FileStorageAdapter due to the
      checks in createFileAdapter

      -    return createOnDemandDbContext(schema, adapter as FileStorageAdapter,
      core);

      +    const fileAdapter = adapter as FileStorageAdapter; // We can be sure
      it's a FileStorageAdapter due to checks

      +    if (fileAdapter.options.multi) {

      +      return createMultiFileOnDemandDbContext(schema, fileAdapter, core);

      +    }

      +    if (fileAdapter.options.perRecord) {

      +      return createPerRecordOnDemandDbContext(schema, fileAdapter, core);

      +    }

      +    throw new KonroError("The 'on-demand' mode requires a 'multi-file' or
      'per-record' storage strategy.");
         }
       
         // For in-memory, just combine the core logic with the adapter and I/O methods.
         return {
           ...core,
      -    schema,

      -    adapter,

      +    schema, adapter,
           read: () => adapter.read(schema),
      -    write: (state) => adapter.write(state),

      +    write: (state) => adapter.write(state, schema),
           createEmptyState: () => createEmptyStateImpl(schema),
         } as InMemoryDbContext<S>;
       }
    patchStrategy: new-unified
snapshot:
  src/adapter.ts: >-
    import path from 'path';

    import type { DatabaseState, TableState } from './types';

    import { createEmptyState } from './operations';

    import type { ColumnDefinition, KonroSchema } from './schema';

    import { type Serializer, getSerializer } from './utils/serializer.util';

    import { FsProvider, defaultFsProvider, writeAtomic } from './fs';

    import { KonroError, KonroStorageError } from './utils/error.util';


    export interface StorageAdapter {
      read<S extends KonroSchema<any, any>>(schema: S): Promise<DatabaseState<S>>;
      write(state: DatabaseState<any>): Promise<void>;
      readonly mode: 'in-memory' | 'on-demand';
    }


    export interface FileStorageAdapter extends StorageAdapter {
      readonly options: FileAdapterOptions;
      readonly fs: FsProvider;
      readonly serializer: Serializer;
      readonly fileExtension: string;
    }


    type SingleFileStrategy = { single: { filepath: string }; multi?: never };

    type MultiFileStrategy = { multi: { dir: string }; single?: never };


    export type FileAdapterOptions = {
      format: 'json' | 'yaml' | 'csv' | 'xlsx';
      fs?: FsProvider;
      /**
       * Defines the data access strategy.
       * - `in-memory`: (Default) Loads the entire database into memory on read. Fast for small/medium datasets.
       * - `on-demand`: Reads from the file system for each query. Slower but supports larger datasets. Requires the 'multi-file' strategy.
       */
      mode?: 'in-memory' | 'on-demand';
    } & (SingleFileStrategy | MultiFileStrategy);


    export function createFileAdapter(options: FileAdapterOptions & { mode:
    'on-demand' }): FileStorageAdapter & { mode: 'on-demand' };

    export function createFileAdapter(options: FileAdapterOptions & { mode?:
    'in-memory' | undefined }): FileStorageAdapter & { mode: 'in-memory' };

    export function createFileAdapter(options: FileAdapterOptions):
    FileStorageAdapter;

    export function createFileAdapter(options: FileAdapterOptions):
    FileStorageAdapter {
      const serializer = getSerializer(options.format);
      const fileExtension = `.${options.format}`;
      const fs = options.fs ?? defaultFsProvider;
      const mode = options.mode ?? 'in-memory';

      const isTabular = options.format === 'csv' || options.format === 'xlsx';
      if (isTabular && (mode !== 'on-demand' || !options.multi)) {
        throw KonroError(`The '${options.format}' format only supports 'on-demand' mode with a 'multi-file' strategy.`);
      }

      if (mode === 'on-demand' && options.single) {
        throw KonroError("The 'on-demand' mode requires the 'multi-file' storage strategy.");
      }

      const parseFile = async <T>(filepath: string, schema?: Record<string, ColumnDefinition<any>>): Promise<T | undefined> => {
        const data = await fs.readFile(filepath);
        if (!data) return undefined;
        try {
          return serializer.parse<T>(data, schema);
        } catch (e: any) {
          throw KonroStorageError(`Failed to parse file at "${filepath}". It may be corrupt or not a valid ${options.format} file. Original error: ${e.message}`);
        }
      };

      const readSingle = async <S extends KonroSchema<any, any>>(schema: S): Promise<DatabaseState<S>> => {
        const state = await parseFile<DatabaseState<any>>(options.single!.filepath);
        // The cast is acceptable as the original code made the same implicit assumption.
        return (state ?? createEmptyState(schema)) as DatabaseState<S>;
      };

      const readMulti = async <S extends KonroSchema<any, any>>(schema: S): Promise<DatabaseState<S>> => {
        const dir = options.multi!.dir;
        await fs.mkdir(dir, { recursive: true });
        const state = createEmptyState(schema);
        await Promise.all(
          Object.keys(schema.tables).map(async (tableName) => {
            const filepath = path.join(dir, `${tableName}${fileExtension}`);
            const tableState = await parseFile<TableState<any>>(filepath, schema.tables[tableName]);
            if (tableState) (state as any)[tableName] = tableState;
          })
        );
        return state;
      };

      const writeSingle = (state: DatabaseState<any>) => writeAtomic(options.single!.filepath, serializer.stringify(state), fs);

      const writeMulti = async (state: DatabaseState<any>) => {
        const dir = options.multi!.dir;
        await fs.mkdir(dir, { recursive: true });
        const writes = Object.entries(state).map(([tableName, tableState]) => {
          const filepath = path.join(dir, `${tableName}${fileExtension}`);
          return writeAtomic(filepath, serializer.stringify(tableState), fs);
        });
        await Promise.all(writes);
      };

      return {
        options,
        fs,
        serializer,
        fileExtension,
        mode,
        read: options.single ? readSingle : readMulti,
        write: options.single ? writeSingle : writeMulti,
      } as FileStorageAdapter;
    }
  src/db.ts: >-
    import path from 'path';

    import { AggregationDefinition, ColumnDefinition, KonroSchema,
    RelationDefinition } from './schema';

    import { StorageAdapter, FileStorageAdapter } from './adapter';

    import { DatabaseState, KRecord, TableState } from './types';

    import { _queryImpl, _insertImpl, _updateImpl, _deleteImpl, createEmptyState
    as createEmptyStateImpl, QueryDescriptor, _aggregateImpl,
    AggregationDescriptor } from './operations';

    import { createPredicateFromPartial } from './utils/predicate.util';

    import { KonroError, KonroStorageError } from './utils/error.util';

    import { writeAtomic } from './fs';


    // A helper to normalize a predicate argument

    const normalizePredicate = <T extends KRecord>(
      predicate: Partial<T> | ((record: T) => boolean)
    ): ((record: KRecord) => boolean) =>
      // The cast is necessary due to function argument contravariance.
      // The internal operations work on the wider `KRecord`, while the fluent API provides the specific `T`.
      (typeof predicate === 'function' ? predicate : createPredicateFromPartial(predicate)) as (record: KRecord) => boolean;

    // --- TYPE HELPERS for Fluent API ---


    type RelatedModel<T> = T extends (infer R)[] ? R : T extends (infer R |
    null) ? R : T;


    // TAll is the full relational model type, e.g. schema.types.users

    type WithArgument<TAll> = { // e.g. TAll = S['types']['users']
      [K in keyof TAll as NonNullable<TAll[K]> extends any[] | object ? K : never]?: boolean | ({
        where?: (record: RelatedModel<NonNullable<TAll[K]>>) => boolean;
      } & (
        | { select: Record<string, ColumnDefinition<unknown>>; with?: never }
        | { select?: never; with?: WithArgument<RelatedModel<NonNullable<TAll[K]>>> }
      ));
    };


    type ResolveWith<
      S extends KonroSchema<any, any>,
      TName extends keyof S['tables'],
      TWith extends WithArgument<S['types'][TName]>
    > = { // TName='users', TWith={posts: {with: {author: true}}}
        [K in keyof TWith & keyof S['relations'][TName]]:
            S['relations'][TName][K] extends { relationType: 'many' }
                ? ( // 'many' relation -> array result. K = 'posts'
                    TWith[K] extends { select: infer TSelect }
                        ? ({ [P in keyof TSelect]: InferColumnType<TSelect[P]> })[]
                        : TWith[K] extends { with: infer TNestedWith }
                            // S['relations']['users']['posts']['targetTable'] = 'posts'
                            ? (S['base'][S['relations'][TName][K]['targetTable']] & ResolveWith<S, S['relations'][TName][K]['targetTable'], TNestedWith & WithArgument<S['types'][S['relations'][TName][K]['targetTable']]>>)[]
                            // posts: true.
                            : S['base'][S['relations'][TName][K]['targetTable']][]
                  )
                : S['relations'][TName][K] extends { relationType: 'one' }
                    ? ( // 'one' relation -> nullable object result
                        TWith[K] extends { select: infer TSelect }
                            ? ({ [P in keyof TSelect]: InferColumnType<TSelect[P]> }) | null
                            : TWith[K] extends { with: infer TNestedWith }
                                ? (S['base'][S['relations'][TName][K]['targetTable']] & ResolveWith<S, S['relations'][TName][K]['targetTable'], TNestedWith & WithArgument<S['types'][S['relations'][TName][K]['targetTable']]>>) | null
                                : S['base'][S['relations'][TName][K]['targetTable']] | null
                      )
                    : never
    };


    // InferColumnType is not exported from schema, so we need it here too.

    type InferColumnType<C> = C extends ColumnDefinition<infer T> ? T : never;


    // --- IN-MEMORY API TYPES (STATEFUL) ---


    interface ChainedQueryBuilder<S extends KonroSchema<any, any>, TName extends
    keyof S['tables'], TReturn> {
      select(fields: Record<string, ColumnDefinition<unknown> | RelationDefinition>): this;
      where(predicate: Partial<S['base'][TName]> | ((record: S['base'][TName]) => boolean)): this;
      with<W extends WithArgument<S['types'][TName]>>(relations: W): ChainedQueryBuilder<S, TName, TReturn & ResolveWith<S, TName, W>>;
      limit(count: number): this;
      offset(count: number): this;
      all(): TReturn[];
      first(): TReturn | null;
      aggregate<TAggs extends Record<string, AggregationDefinition>>(
        aggregations: TAggs
      ): { [K in keyof TAggs]: number | null };
    }


    interface QueryBuilder<S extends KonroSchema<any, any>> {
      from<T extends keyof S['tables']>(tableName: T): ChainedQueryBuilder<S, T, S['base'][T]>;
    }


    interface UpdateBuilder<S extends KonroSchema<any, any>, TBase, TCreate> {
      set(data: Partial<TCreate>): {
        where(predicate: Partial<TBase> | ((record: TBase) => boolean)): [DatabaseState<S>, TBase[]];
      };
    }


    interface DeleteBuilder<S extends KonroSchema<any, any>, TBase> {
      where(predicate: Partial<TBase> | ((record: TBase) => boolean)): [DatabaseState<S>, TBase[]];
    }


    export interface InMemoryDbContext<S extends KonroSchema<any, any>> {
      schema: S;
      adapter: StorageAdapter;
      read(): Promise<DatabaseState<S>>;
      write(state: DatabaseState<S>): Promise<void>;
      createEmptyState(): DatabaseState<S>;

      query(state: DatabaseState<S>): QueryBuilder<S>;
      insert<T extends keyof S['tables']>(state: DatabaseState<S>, tableName: T, values: S['create'][T]): [DatabaseState<S>, S['base'][T]];
      insert<T extends keyof S['tables']>(state: DatabaseState<S>, tableName: T, values: Readonly<S['create'][T]>[]): [DatabaseState<S>, S['base'][T][]];
      update<T extends keyof S['tables']>(state: DatabaseState<S>, tableName: T): UpdateBuilder<S, S['base'][T], S['create'][T]>;
      delete<T extends keyof S['tables']>(state: DatabaseState<S>, tableName: T): DeleteBuilder<S, S['base'][T]>;
    }



    // --- ON-DEMAND API TYPES (STATELESS & ASYNC) ---


    interface OnDemandChainedQueryBuilder<S extends KonroSchema<any, any>, TName
    extends keyof S['tables'], TReturn> {
      select(fields: Record<string, ColumnDefinition<unknown> | RelationDefinition>): this;
      where(predicate: Partial<S['base'][TName]> | ((record: S['base'][TName]) => boolean)): this;
      with<W extends WithArgument<S['types'][TName]>>(relations: W): OnDemandChainedQueryBuilder<S, TName, TReturn & ResolveWith<S, TName, W>>;
      limit(count: number): this;
      offset(count: number): this;
      all(): Promise<TReturn[]>;
      first(): Promise<TReturn | null>;
      aggregate<TAggs extends Record<string, AggregationDefinition>>(
        aggregations: TAggs
      ): Promise<{ [K in keyof TAggs]: number | null }>;
    }


    interface OnDemandQueryBuilder<S extends KonroSchema<any, any>> {
      from<T extends keyof S['tables']>(tableName: T): OnDemandChainedQueryBuilder<S, T, S['base'][T]>;
    }


    interface OnDemandUpdateBuilder<TBase, TCreate> {
      set(data: Partial<TCreate>): {
        where(predicate: Partial<TBase> | ((record: TBase) => boolean)): Promise<TBase[]>;
      };
    }


    interface OnDemandDeleteBuilder<TBase> {
      where(predicate: Partial<TBase> | ((record: TBase) => boolean)): Promise<TBase[]>;
    }


    export interface OnDemandDbContext<S extends KonroSchema<any, any>> {
      schema: S;
      adapter: StorageAdapter;
      read(): Promise<never>; // Not supported in on-demand mode
      write(): Promise<never>; // Not supported in on-demand mode
      createEmptyState(): DatabaseState<S>;

      query(): OnDemandQueryBuilder<S>;
      insert<T extends keyof S['tables']>(tableName: T, values: S['create'][T]): Promise<S['base'][T]>;
      insert<T extends keyof S['tables']>(tableName: T, values: Readonly<S['create'][T]>[]): Promise<S['base'][T][]>;
      update<T extends keyof S['tables']>(tableName: T): OnDemandUpdateBuilder<S['base'][T], S['create'][T]>;
      delete<T extends keyof S['tables']>(tableName: T): OnDemandDeleteBuilder<S['base'][T]>;
    }


    // --- DbContext Union Type ---

    export type DbContext<S extends KonroSchema<any, any>> =
    InMemoryDbContext<S> | OnDemandDbContext<S>;


    // --- CORE LOGIC (STATELESS & PURE) ---


    /**
     * Creates the core, stateless database operations.
     * These operations are pure functions that take a database state and return a new state,
     * forming the foundation for both in-memory and on-demand modes.
     */
    function createCoreDbContext<S extends KonroSchema<any, any>>(schema: S) {
      const query = (state: DatabaseState<S>): QueryBuilder<S> => ({
        from: <TName extends keyof S['tables']>(tableName: TName): ChainedQueryBuilder<S, TName, S['base'][TName]> => {
          const createBuilder = <TReturn>(currentDescriptor: QueryDescriptor): ChainedQueryBuilder<S, TName, TReturn> => ({
            select(fields) { return createBuilder<TReturn>({ ...currentDescriptor, select: fields }); },
            where(predicate) { return createBuilder<TReturn>({ ...currentDescriptor, where: normalizePredicate(predicate as any) }); },
            with<W extends WithArgument<S['types'][TName]>>(relations: W) {
              const newWith = { ...currentDescriptor.with, ...(relations as QueryDescriptor['with']) };
              return createBuilder<TReturn & ResolveWith<S, TName, W>>({ ...currentDescriptor, with: newWith });
            },
            limit(count) { return createBuilder<TReturn>({ ...currentDescriptor, limit: count }); },
            offset(count) { return createBuilder<TReturn>({ ...currentDescriptor, offset: count }); },
            all: (): TReturn[] => _queryImpl(state as DatabaseState, schema, currentDescriptor) as any,
            first: (): TReturn | null => (_queryImpl(state as DatabaseState, schema, { ...currentDescriptor, limit: 1 })[0] ?? null) as any,
            aggregate: (aggregations) => {
              const aggDescriptor: AggregationDescriptor = { ...currentDescriptor, aggregations };
              return _aggregateImpl(state as DatabaseState, schema, aggDescriptor) as any;
            },
          });
          return createBuilder<S['base'][TName]>({ tableName: tableName as string });
        },
      });

      const insert = <T extends keyof S['tables']>(
        state: DatabaseState<S>, tableName: T, values: S['create'][T] | Readonly<S['create'][T]>[]
      ): [DatabaseState<S>, S['base'][T] | S['base'][T][]] => {
        const valsArray = Array.isArray(values) ? values : [values];
        const [newState, inserted] = _insertImpl(state as DatabaseState, schema, tableName as string, valsArray as KRecord[]);
        const result = Array.isArray(values) ? inserted : inserted[0];
        return [newState as DatabaseState<S>, result] as [DatabaseState<S>, S['base'][T] | S['base'][T][]];
      };

      const update = <T extends keyof S['tables']>(state: DatabaseState<S>, tableName: T): UpdateBuilder<S, S['base'][T], S['create'][T]> => ({
        set: (data) => ({
          where: (predicate) => {
            const [newState, updatedRecords] = _updateImpl(state as DatabaseState, schema, tableName as string, data as Partial<KRecord>, normalizePredicate(predicate as any));
            return [newState as DatabaseState<S>, updatedRecords as S['base'][T][]];
          },
        }),
      });

      const del = <T extends keyof S['tables']>(state: DatabaseState<S>, tableName: T): DeleteBuilder<S, S['base'][T]> => ({
        where: (predicate) => {
          const [newState, deletedRecords] = _deleteImpl(state as DatabaseState, tableName as string, normalizePredicate(predicate as any));
          return [newState as DatabaseState<S>, deletedRecords as S['base'][T][]];
        },
      });

      return { query, insert, update, delete: del };
    }


    // --- ON-DEMAND CONTEXT (STATEFUL WRAPPER) ---


    function createOnDemandDbContext<S extends KonroSchema<any, any>>(
      schema: S,
      adapter: FileStorageAdapter,
      core: ReturnType<typeof createCoreDbContext<S>>
    ): OnDemandDbContext<S> {
      const { dir } = adapter.options.multi!;

      const readTable = async (tableName: string): Promise<TableState> => {
        const filepath = path.join(dir, `${tableName}${adapter.fileExtension}`);
        const data = await adapter.fs.readFile(filepath);
        if (!data) return { records: [], meta: { lastId: 0 } };
        try {
          return adapter.serializer.parse(data, schema.tables[tableName]);
        } catch (e: any) {
          throw KonroStorageError(`Failed to parse file at "${filepath}". It may be corrupt or not a valid ${adapter.options.format} file. Original error: ${e.message}`);
        }
      };

      const writeTable = async (tableName: string, tableState: TableState): Promise<void> => {
        await adapter.fs.mkdir(dir, { recursive: true });
        const filepath = path.join(dir, `${tableName}${adapter.fileExtension}`);
        const content = adapter.serializer.stringify(tableState);
        await writeAtomic(filepath, content, adapter.fs);
      };
      
      // For queries with relations, we need the full state.
      const getFullState = async (): Promise<DatabaseState<S>> => {
        const state = createEmptyStateImpl(schema);
        await Promise.all(Object.keys(schema.tables).map(async (tableName) => {
          (state as any)[tableName] = await readTable(tableName);
        }));
        return state;
      }

      // A generic handler for CUD operations that reads one table, performs an action, and writes it back.
      const performCud = async <TResult>(tableName: string, action: (state: DatabaseState<S>) => [DatabaseState<S>, TResult]): Promise<TResult> => {
        const state = createEmptyStateImpl(schema);
        (state as any)[tableName] = await readTable(tableName);
        const [newState, result] = action(state);
        
        // Check if the operation produced a result (e.g., an array of inserted/updated/deleted records)
        const hasChanges = Array.isArray(result) ? result.length > 0 : result !== null;
        if (hasChanges) {
          const newTableState = newState[tableName as string];
          // This check satisfies the `noUncheckedIndexedAccess` compiler option.
          // Our CUD logic ensures this state will always exist after a change.
          if (newTableState) {
            await writeTable(tableName, newTableState);
          }
        }
        return result;
      };

      const query = (): OnDemandQueryBuilder<S> => ({
        from: <TName extends keyof S['tables']>(tableName: TName): OnDemandChainedQueryBuilder<S, TName, S['base'][TName]> => {
          // The query builder for on-demand must be separate because its terminal methods are async.
          const createBuilder = <TReturn>(currentDescriptor: QueryDescriptor): OnDemandChainedQueryBuilder<S, TName, TReturn> => ({
            select(fields) { return createBuilder<TReturn>({ ...currentDescriptor, select: fields }); },
            where(predicate) { return createBuilder<TReturn>({ ...currentDescriptor, where: normalizePredicate(predicate as any) }); },
            with<W extends WithArgument<S['types'][TName]>>(relations: W) {
              const newWith = { ...currentDescriptor.with, ...(relations as QueryDescriptor['with']) };
              return createBuilder<TReturn & ResolveWith<S, TName, W>>({ ...currentDescriptor, with: newWith });
            },
            limit(count) { return createBuilder<TReturn>({ ...currentDescriptor, limit: count }); },
            offset(count) { return createBuilder<TReturn>({ ...currentDescriptor, offset: count }); },
            all: async (): Promise<TReturn[]> => {
              const state = await getFullState();
              return _queryImpl(state, schema, currentDescriptor) as any;
            },
            first: async (): Promise<TReturn | null> => {
              const state = await getFullState();
              return (_queryImpl(state, schema, { ...currentDescriptor, limit: 1 })[0] ?? null) as any;
            },
            aggregate: async (aggregations) => {
              const state = await getFullState();
              const aggDescriptor: AggregationDescriptor = { ...currentDescriptor, aggregations };
              return _aggregateImpl(state, schema, aggDescriptor) as any;
            },
          });
          return createBuilder<S['base'][TName]>({ tableName: tableName as string });
        },
      });

      const insert = <T extends keyof S['tables']>(tableName: T, values: S['create'][T] | Readonly<S['create'][T]>[]): Promise<any> => 
        performCud(tableName as string, (state) => core.insert(state, tableName, values as any));

      const update = <T extends keyof S['tables']>(tableName: T): OnDemandUpdateBuilder<S['base'][T], S['create'][T]> => ({
        set: (data) => ({
          where: (predicate) => performCud(tableName as string, (state) => core.update(state, tableName).set(data).where(predicate as any)) as Promise<S['base'][T][]>,
        }),
      });

      const del = <T extends keyof S['tables']>(tableName: T): OnDemandDeleteBuilder<S['base'][T]> => ({
        where: (predicate) => performCud(tableName as string, (state) => core.delete(state, tableName).where(predicate as any)) as Promise<S['base'][T][]>,
      });

      const notSupported = () => Promise.reject(KonroError("This method is not supported in 'on-demand' mode."));

      return {
        schema,
        adapter,
        read: notSupported,
        write: notSupported,
        createEmptyState: () => createEmptyStateImpl(schema),
        query,
        insert,
        update,
        delete: del,
      };
    }



    // --- DATABASE FACTORY ---


    export function createDatabase<
      S extends KonroSchema<any, any>,
      TAdapter extends StorageAdapter,
    >(
      options: { schema: S; adapter: TAdapter }
    ): TAdapter['mode'] extends 'on-demand' ? OnDemandDbContext<S> :
    InMemoryDbContext<S>;

    export function createDatabase<S extends KonroSchema<any, any>>(
      options: { schema: S; adapter: StorageAdapter }
    ): DbContext<S> {
      const { schema, adapter } = options;
      const core = createCoreDbContext(schema);

      if (adapter.mode === 'on-demand') {
        // We can be reasonably sure it's a FileStorageAdapter due to the checks in createFileAdapter
        return createOnDemandDbContext(schema, adapter as FileStorageAdapter, core);
      }

      // For in-memory, just combine the core logic with the adapter and I/O methods.
      return {
        ...core,
        schema,
        adapter,
        read: () => adapter.read(schema),
        write: (state) => adapter.write(state),
        createEmptyState: () => createEmptyStateImpl(schema),
      } as InMemoryDbContext<S>;
    }
approved: true
